1.发现最优轨迹可能是一个绕来绕去甚至回头的轨迹：所以我们修改了save_best_trajectory()函数
只在智能体找到一条更好的路径（即获得更高奖励）且该路径是连续的、没有回头或重复的情况下才保存为最佳轨迹。
（这一步我觉得如果训练的足够好可以去掉，不应该多此一步，这是对自己训练设置的不自信：应该直接根据奖励来判断）

2.修改MAX_STEPS和EPISODES
MAX_STEPS要让智能体足够找到最优路径，并且避免设置过大智能体过度游走浪费计算资源；EPISODES的设置要让智能体有足够的机会去探索学习并且Q值收敛
EPISODES = 500
MAX_STEPS = 50
GRID_SIZE = 5
<!-- # - EPISODES = 500 ：
# - 增加训练回合数可以让智能体有更多机会探索和学习
# - 由于环境相对简单（5x5网格），500回合足够让Q值收敛
# - 可以更好地观察奖励曲线的趋势和学习效果

# - MAX_STEPS = 50 ：
# - 在5x5的网格中，从起点到终点的最优路径不会超过10步
# - 设置50步足够智能体在一个回合内探索和找到目标
# - 过大的步数（如100）可能导致：
#   - 智能体在一个回合中过度游走
#   - 浪费计算资源
#   - 增加学习无效经验的可能性 -->

3.发现智能体在训练后期起始点总是向右走而不是向上走：调整epsilon的衰减率更大使得探索率降低速度更慢
<!-- # 在训练后期，智能体的探索率（epsilon）已经降低到很小的值，导致它更倾向于利用已学到的经验而不是探索新的路径。
# 在这种情况下，如果智能体之前学到了一些不太理想的行为模式（比如在起点和右边障碍物之间来回移动），它可能会陷入这种局部最优解。
# 这表明我们可能需要调整epsilon的衰减率或者增加训练的回合数，让智能体有更多机会探索不同的路径。 -->

4.发现智能体在训练前期就开始起始点总是向右走而不是向上走：给于碰障碍物更大的负面奖励（重点）     
<!-- # 调整奖励函数的设置，使得碰障碍物会获得较大的负面奖励
# 给予到达终点更大的奖励值，鼓励到达终点（并且最优轨迹依据奖励来判断，我们优先选择到达终点的轨迹为最优轨迹） -->

5.智能体总是碰墙壁：设置碰到边界负面奖励值，和碰障碍物一样

6.修改碰到障碍物和边界直接回到起点，而是返回原处：这避免了比较远的点来不及训练就回到起点了（重点）

7.调参：降低学习率、增大目标网络更新频率、增大经验缓冲区；确保loss不是一直增大

8.可视化调整：
1.# 每一步结束都可视化当前状态和Q值分布，而EPISODES = 500，MAX_STEPS = 50，程序跑这么快，人的肉眼能看到每一步的可视化吗
<!-- # 所以当前操作下每一步的训练完要等0.1s，MAX_STEPS = 50也就是5s训练完一个回合，要2500s才能训练完整个过程 --> -->
<!-- 2. 代表q值大小的四个小箭头起始点要在一个点，也就是网格的中心
3. 所有的训练完以后给出两个图，1.所有状态所有方向上Q值的可视化的图，不要含轨迹 2.最优轨迹的图 -->

9.考虑增加每一轮的训练步数由50到100，不然智能体太菜了一直找不到最佳路径/或者增加训练轮数：成功找到路径

10.智能体很少在前期能找到后面的状态和动作，所以无法学习其的Q值；到后期完全依据经验而不是探索了，就困难很大：减小探索率衰减速度，并且增加回合数
增加训练回合数500到1000：我踏马悟了，加大训练量就没有完不成的事情
